{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd59b1d-ee48-46fa-82c5-ffe64194e027",
   "metadata": {},
   "source": [
    "# Data storage and formats\n",
    "\n",
    "In this notebook, we will learn about cloud storage and the Parquet format for storing tabular data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d2b79-653c-4069-a0b3-7598c218b5e2",
   "metadata": {},
   "source": [
    "Large datasets are typically stored on cloud object storage, that are designed to:\n",
    "* store massive files,\n",
    "* for long periods of time, and\n",
    "* support parallelism in I/O operations.\n",
    "\n",
    "Some of the largest providers of object storage are Amazon S3, Google Cloud Storage, and Azure Data Lake. In this tutorial, we're accessing data stored on Google Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ef666-e482-4e1e-aaf5-7394a5019009",
   "metadata": {},
   "source": [
    "## Data has gravity\n",
    "\n",
    "It's almost always better to move your computations to the data (compared to vice-versa). This is because data transfers are typically the highest bottlenecks, so downloading data to a local machine and then computing will be very slow, even for small amounts of data.\n",
    "\n",
    "If your data is stored locally (for example, on hard drives), you should consider a local / on-premise cluster setup.\n",
    "\n",
    "If your data is stored on the cloud, you can spin up a cluster on the same cloud. Note that moving data between cloud providers can also get challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8a37f-6122-40c3-be87-c4c8e656ceda",
   "metadata": {},
   "source": [
    "## Cloud storage as file systems\n",
    "\n",
    "Libraries like `s3fs` and `gcsfs` allow you to access the data with a Python interface. In this tutorial, [we're using `gcsfs`](https://gcsfs.readthedocs.io/en/latest/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a768a-75a0-44a7-b710-794a967c799c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37d87e-9846-4817-a0d9-a473a920561f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1d912-e786-400a-925d-82292cfdf31c",
   "metadata": {},
   "source": [
    "We're accessing public datasets, but you can also pass tokens for private buckets: `GCSFileSystem(token=your_token)`. \n",
    "\n",
    "You can now take a look at the storage bucket in a file-system like interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfff64-bfd8-44f3-950b-09e64831c0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"quansight-datasets/airline-ontime-performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca6ccb-b6cd-43d2-8346-fbfb84148f27",
   "metadata": {},
   "source": [
    "### ðŸ’» Your turn: Open the above folders to view the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5ea50-e752-4083-947e-82f6eb20c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. When ready, click on the three dots below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe0b0c-1368-4776-ba82-8c680ae9f598",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"quansight-datasets/airline-ontime-performance/csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10947e8c-ce3e-4864-8e04-baacd35b7aac",
   "metadata": {},
   "source": [
    "### ðŸ’» Your turn: Read a line from one of the CSV files\n",
    "\n",
    "Hint: How would you do it if it was a local file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ce682-13a3-4714-b323-c003cfaa4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. When ready, click on the three dots below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1de8c-5b83-4585-a9d7-131be1e14b1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with fs.open(\"quansight-datasets/airline-ontime-performance/csv/bts_airline_ontime_performance_april_2003.csv\", \"r\") as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b3c4f-7b3b-4dba-a39c-a6dcb606c173",
   "metadata": {},
   "source": [
    "## Start a Dask Gateway cluster\n",
    "\n",
    "This time, let's specify the various options explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99a28e-226d-45fa-94c9-ee78c2688d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask_gateway\n",
    "\n",
    "gateway = dask_gateway.Gateway()\n",
    "\n",
    "options = gateway.cluster_options(use_local_defaults=False)\n",
    "options.profile = \"Medium Worker\"\n",
    "options.conda_environment = \"global/global-data-of-unusual-size\"\n",
    "\n",
    "cluster = gateway.new_cluster(options)\n",
    "\n",
    "cluster.adapt(minimum=5, maximum=10)\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d93a0-eef5-4b58-bbb6-eee4f05ab320",
   "metadata": {},
   "source": [
    "Make sure to open the following plots: Cluster map, task stream, progress, and workers memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488059d-00b1-4c04-82a7-ad4e5887ea71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CSV data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd76700-fef9-4318-ab4a-90bfa9fab803",
   "metadata": {},
   "source": [
    "We'll download the CSV files again, as we did in the previous notebook, note the time various operations take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e8bef-d3f5-423b-8f3e-f4985adef5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('prep/dtypes.json', 'r') as f:\n",
    "    dtypes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9bbb3-1dbe-4138-bff1-f3d62ea136cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacdeaf-c5dc-445b-baab-df0acb75068f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf = dd.read_csv(\"gcs://quansight-datasets/airline-ontime-performance/csv/*\", dtype=dtypes) # Wall time: 20.2 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8ed2a-c668-4b6e-a72e-bcabf30d6e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "ddf.head() # Wall time: 8.62 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e731e9d-6f68-4702-af9f-505b45716a44",
   "metadata": {},
   "source": [
    "### ðŸ’» Your turn: Compute the number of unique flights taken each year\n",
    "\n",
    "Make sure to time it, and watch the dashboard plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c8426-09a4-4785-b561-12f711bb42a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here. When ready, click on the three dots below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33897d12-f906-4e00-80de-fd67b551629a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf.groupby('YEAR').OP_UNIQUE_CARRIER.count().compute() # Wall time: 4min 17s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085d726-8d91-48c9-8790-fd6ccd056884",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parquet data format\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/) is a columnar data format widely used for storing large tabular datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42ff38-a69b-4e94-92da-b2e9494f0242",
   "metadata": {},
   "source": [
    "### Parquet I/O\n",
    "\n",
    "Parquet data is very efficient to store and access (i.e., compression and encoding), and stores metadata like data-types, column names, and ranges per file/partition.\n",
    "\n",
    "In the following cells we read the full parquet dataset, notice how it's faster and that we did not need to explicitly share datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abdc5f0-87d6-4e03-be4f-47fa1e567158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf_pq = dd.read_parquet(\"gcs://quansight-datasets/airline-ontime-performance/full_dataset.parquet\") # Wall time: 1.05 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488340ba-7e75-4845-abf6-262ca014b9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf_pq.head() # Wall time: 1.16 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da590b9-8e85-4a1e-99f0-49870abcb38b",
   "metadata": {},
   "source": [
    "### ðŸ’» Your turn: Perform same computation as earlier to compute the number of unique flights taken each year\n",
    "\n",
    "Time it again and compare against the previous value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053e46d-ffa9-4186-8556-368dbe99790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. When ready, click on the three dots below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9f300-5a1c-44a2-9348-57c9cbb7124b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf_pq.groupby('YEAR').OP_UNIQUE_CARRIER.count().compute() # Wall time: 14.1 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450a3d7-68ab-4deb-b782-d60303aa8b7f",
   "metadata": {},
   "source": [
    "### Read specific columns\n",
    "\n",
    "As a column-oriented format, you can decide to read only necessary columns, further improving efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15919cf-5900-473c-a589-d9dd92f5faac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf_pq_subset = dd.read_parquet(\"gcs://quansight-datasets/airline-ontime-performance/full_dataset.parquet\",\n",
    "                                  columns= ['YEAR', 'OP_UNIQUE_CARRIER']) # Wall time: 400 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37419742-f821-474e-ae23-044cde1ab662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf_pq_subset.groupby('YEAR').OP_UNIQUE_CARRIER.count().compute() # Wall time: 12.9 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d689a2-422f-4648-91cc-ba97a4f7ad8d",
   "metadata": {},
   "source": [
    "### Partitioned storage\n",
    "\n",
    "Parquet files can be stored with a partitioning schema that works best for your computation.\n",
    "\n",
    "It's useful to take the time to partition your dataset based on your workflows (partition structure, as well as number of partitions). Dask can partition your DataFrame accordingly when you read the data.\n",
    "\n",
    "Here, we've partitioned the dataset by `YEAR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec3409-b4cd-4f66-a068-1e3f7545b412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"quansight-datasets/airline-ontime-performance/parquet_by_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daf3f2-6425-45db-bc0f-055308521bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"quansight-datasets/airline-ontime-performance/parquet_by_year/YEAR=2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78914cef-8e4f-46c1-acb2-36a6d529fd27",
   "metadata": {},
   "source": [
    "### Row-wise filtering\n",
    "\n",
    "Parquet also stores the ranges of values present in each file and partition, so you can efficiently filter Parquet datasets row-wise while reading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da698525-b7fb-4e65-952a-73e05818e92d",
   "metadata": {},
   "source": [
    "For example, consider we want to exclude 2020 because of its unique impact on the airline industry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a9de38-393a-4530-9b9f-8b55230db146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf_pq_subset = dd.read_parquet(\"gcs://quansight-datasets/airline-ontime-performance/full_dataset.parquet\",\n",
    "                                  columns= ['YEAR', 'OP_UNIQUE_CARRIER'],\n",
    "                                  filters = [[('YEAR', '!=', 2020)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da3e05-35c3-491a-9854-94d2a517201e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf_pq_subset.groupby('YEAR').OP_UNIQUE_CARRIER.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c1685-1445-4caf-a636-88cb067b497d",
   "metadata": {},
   "source": [
    "#### ðŸ’» Your turn: Groupby month instead of year, and only read+calculate unique flights for Q4 of each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf620867-a1f0-460a-9948-4d0bd32cfb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here. When ready, click on the three dots below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6380f2a-068a-400a-afa1-8e4986feb38c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf_pq_q4 = dd.read_parquet(\"gcs://quansight-datasets/airline-ontime-performance/full_dataset.parquet\",\n",
    "                                  columns= ['MONTH', 'OP_UNIQUE_CARRIER'],\n",
    "                                  filters = [[('MONTH', '>=', 9)]])\n",
    "\n",
    "ddf_pq_q4.groupby('MONTH').OP_UNIQUE_CARRIER.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3c8ba-fd31-4fcd-b99e-8182a9236bb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Convert from CSV to Parquet\n",
    "\n",
    "You can convert CSV files to Parquet in two main ways:\n",
    "\n",
    "- Dask (and pandas) have a `to_parquet()` function, you can also partition the data while converting.\n",
    "- You can use powerful Parquet engines like [`pyarrow`](https://arrow.apache.org/docs/python/csv.htmlhttps://arrow.apache.org/docs/python/csv.html) or `fastparquet` directly (Dask and pandas use these engines internally)\n",
    "\n",
    "```python\n",
    "\n",
    "# You do not need to execute this code\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dd.read_csv(\"path_to_csv_files_on_cloud_storage\")\n",
    "dd.to_parquet(\"path_to_cloud_storage_loaction\", partition_by=\"\")\n",
    "```\n",
    "\n",
    "We created the Parquet dataset using Dask, and our code is available in `scripts/csv_to_parquet.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d85bf9-1a08-472d-96fe-24dff1d5b60b",
   "metadata": {},
   "source": [
    "## Notable mentions\n",
    "\n",
    "* If you're working with multidimensional arrays, [Zarr](https://zarr.readthedocs.io/en/stable/index.html) is an excellent format to store chunked array data (similar to partitioning, but along multiple dimensions).\n",
    "* If you expect your workflows to have SQL-like query operations, storing your data in [Snowflake](https://www.snowflake.com/en/) can be a good option.\n",
    "* [Creating Disk Partitioned Lakes with Dask using partition_on](https://www.coiled.io/blog/dask-disk-partition-on), a blog post by Coiled, has some valuable best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65cc9d-22e3-42e1-a63a-fba819cf2ca6",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Next â†’\n",
    "\n",
    "[Big data analysis with Dask](./05-big-data-analysis-with-dask.ipynb)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97c3fe-11e5-40ec-a074-a0bc73b92261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-data-of-unusual-size",
   "language": "python",
   "name": "conda-env-global-global-data-of-unusual-size-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
